{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from ete3 import Tree\n",
    "\n",
    "class TreeEncoder:\n",
    "\n",
    "    def encode_tree(self, tree_str):\n",
    "        \"\"\"\n",
    "        Encode the tree structure into a format suitable for input into the neural network.\n",
    "        \"\"\"\n",
    "        # Check if the tree is already encoded\n",
    "        csv_file = tree_str[:-4] + '.csv'\n",
    "        if os.path.exists(csv_file):\n",
    "            return csv_file\n",
    "        # Call the external script to get CDV encoding\n",
    "        cmd = f\"python -m CDV_full_tree -t {tree_str} > {tree_str[:-4]}.csv\"\n",
    "        os.system(cmd)\n",
    "\n",
    "    def encode_all_trees(self, trees_directory):\n",
    "        \"\"\"\n",
    "        Encode all the trees in the given directory.\n",
    "        \"\"\"\n",
    "        tree_files = [os.path.join(trees_directory, file) for file in os.listdir(trees_directory) if file.endswith('.nwk')]\n",
    "        for tree_file in tree_files:\n",
    "            print(tree_file)\n",
    "            self.encode_tree(tree_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trees_directory = \"trees/musse/\"\n",
    "encoder = TreeEncoder()\n",
    "encoded_trees = encoder.encode_all_trees(trees_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\matti\\AppData\\Local\\Temp\\ipykernel_2284\\2448671926.py:25: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  param_df = pd.concat([param_df, pd.DataFrame([lambdas + mus + qs], columns=column_names)], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(245, 12)\n",
      "(245, 502)\n",
      "        1         2         3         4         5         6         7    \\\n",
      "0  6.866735  5.767957  4.863890  3.943322  6.208207  6.504332  6.854792   \n",
      "1  4.675823  3.496983  3.219939  4.381005  4.644829  2.699579  2.623678   \n",
      "2  6.650845  4.359031  6.405473  6.275414  5.912631  4.167767  2.134146   \n",
      "3  8.588370  8.091624  7.742888  7.574931  7.423699  5.879166  7.452799   \n",
      "4  7.745109  6.936440  6.698193  5.980017  7.499169  7.624462  5.845326   \n",
      "\n",
      "        8         9         10   ...  493  494  495  496  497  498  499  500  \\\n",
      "0  3.583706  5.303253  6.417531  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "1  2.427730  3.084728  2.946469  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "2  2.236878  5.775362  0.644143  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "3  7.264826  8.453792  4.531270  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "4  3.787765  5.480250  3.545348  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "\n",
      "   501       502  \n",
      "0  0.0  1.309473  \n",
      "1  0.0  1.425360  \n",
      "2  0.0  1.511337  \n",
      "3  0.0  1.572343  \n",
      "4  0.0  1.046502  \n",
      "\n",
      "[5 rows x 502 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "cutoff = 100  # Number of trees to process\n",
    "\n",
    "# Load parameter values as a dataframe\n",
    "tree_files = [os.path.join(trees_directory, file) for file in os.listdir(trees_directory) if file.endswith('.nwk')]\n",
    "parameter_files = [os.path.join(trees_directory, file[:-4] + '.params') for file in os.listdir(trees_directory) if file.endswith('.nwk')]\n",
    "\n",
    "def process_params_musse(param_file):\n",
    "    with open(param_file, 'r') as f:\n",
    "        params = f.readlines()\n",
    "        num_states = int(params[0].split()[1])\n",
    "        lambdas = [float(row.split()[1]) for row in params[1:1 + num_states]]\n",
    "        mus = [float(row.split()[1]) for row in params[1 + num_states:1 + 2 * num_states]]\n",
    "        qs = [float(row.split()[1]) for row in params[1 + 2 * num_states:]]\n",
    "        return lambdas, mus, qs\n",
    "\n",
    "# Create a dataframe to store the parameter values\n",
    "column_names = ['lambda1', 'lambda2', 'lambda3', 'mu1', 'mu2', 'mu3', 'q12', 'q13', 'q21', 'q23', 'q31', 'q32']\n",
    "param_df = pd.DataFrame(columns=column_names)\n",
    "for param_file in parameter_files:\n",
    "    lambdas, mus, qs = process_params_musse(param_file)\n",
    "    new_row = pd.DataFrame([lambdas + mus + qs], columns=column_names)\n",
    "    \n",
    "    param_df = pd.concat([param_df, pd.DataFrame([lambdas + mus + qs], columns=column_names)], ignore_index=True)   \n",
    "\n",
    "# loading tree encodings/representations\n",
    "# encoding has the following structure: 1 value of tree height, 500 values for tip states ('1' or '2')\n",
    "# 1 value for tree height and 500 values for internal node heights\n",
    "# + 2 values for nb of tips of each type (to be removed) and 1 value of rescaling (removed, but stocked for rescaling predicted values back to the original scale)\n",
    "\n",
    "encoded_tree_files = [os.path.join(trees_directory, file) for file in os.listdir(trees_directory) if file.endswith('.csv')]\n",
    "encoded_trees = pd.DataFrame()\n",
    "for encoded_tree_file in encoded_tree_files:\n",
    "    encoded_tree = pd.read_csv(encoded_tree_file, sep=\"\\t\", header=None, index_col=0, skiprows=1)  # Skip the header row\n",
    "    encoded_trees = pd.concat([encoded_trees, encoded_tree], ignore_index=True)\n",
    "\n",
    "# make sure there is correspondance between indexes of dataframe with parameter values and encodings\n",
    "print(param_df.shape)\n",
    "print(encoded_trees.shape)\n",
    "param_df.index = encoded_trees.index\n",
    "print(encoded_trees.head())\n",
    "\n",
    "# split\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "((sp27:1.250340508,sp28:1.250340508)nd15:7.74146153,((((sp30:0.6971156603,(sp44:0.05448322787,sp45:0.05448322787)nd43:0.6426324324)nd39:0.6993506711,sp26:1.396466331)nd17:4.57236873,(((((sp39:0.2613079878,sp40:0.2613079878)nd40:0.7983119657,(sp31:0.6836493874,sp32:0.6836493874)nd41:0.3759705661)nd36:2.581591996,(sp21:2.047336495,(sp33:0.5882194838,sp34:0.5882194838)nd38:1.459117012)nd30:1.593875454)nd23:0.2542306771,((sp41:0.2506839765,(sp42:0.2329470068,sp43:0.2329470068)nd46:0.01773696969)nd44:0.2999376771,sp35:0.5506216536)nd24:3.344820973)nd22:0.4035940053,((sp16:2.622670915,(sp24:1.438818684,sp25:1.438818684)nd37:1.183852231)nd25:1.205457832,(((sp46:0.01563795384,sp47:0.01563795384)nd45:0.458918388,sp38:0.4745563419)nd42:0.3877672302,sp29:0.862323572)nd32:2.965805175)nd21:0.470907884)nd13:1.66979843)nd10:1.06711515,((((sp22:1.711465293,sp23:1.711465293)nd34:1.935683526,(sp36:0.5114053313,sp37:0.5114053313)nd33:3.135743488)nd19:0.7528428002,sp10:4.39999162)nd14:1.449883434,sp3:5.849875054)nd11:1.186075157)nd6:1.955851826).csv not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[72], line 107\u001b[0m\n\u001b[0;32m    104\u001b[0m tree_files, parameter_files \u001b[38;5;241m=\u001b[39m load_data(trees_directory)\n\u001b[0;32m    106\u001b[0m \u001b[38;5;66;03m# Process data\u001b[39;00m\n\u001b[1;32m--> 107\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtree_files\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameter_files\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;66;03m# Split data into training and testing sets\u001b[39;00m\n\u001b[0;32m    110\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "Cell \u001b[1;32mIn[72], line 83\u001b[0m, in \u001b[0;36mprocess_data\u001b[1;34m(tree_files, parameter_files)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(tree_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     82\u001b[0m     tree_str \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m---> 83\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtree_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncoded tree shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, x\u001b[38;5;241m.\u001b[39mshape)  \u001b[38;5;66;03m# Print the shape of the encoded tree for debugging\u001b[39;00m\n\u001b[0;32m     85\u001b[0m     X\u001b[38;5;241m.\u001b[39mappend(x)\n",
      "Cell \u001b[1;32mIn[72], line 54\u001b[0m, in \u001b[0;36mTreeEncoder.encode_tree\u001b[1;34m(self, tree_str)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# Now read the generated CSV file\u001b[39;00m\n\u001b[0;32m     53\u001b[0m csv_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtree_str[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m4\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 54\u001b[0m encoded_tree \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloadtxt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# Pad or truncate to match max_length\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoded_tree) \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_length:\n",
      "File \u001b[1;32mc:\\Users\\matti\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\lib\\npyio.py:1373\u001b[0m, in \u001b[0;36mloadtxt\u001b[1;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows, quotechar, like)\u001b[0m\n\u001b[0;32m   1370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(delimiter, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[0;32m   1371\u001b[0m     delimiter \u001b[38;5;241m=\u001b[39m delimiter\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatin1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m-> 1373\u001b[0m arr \u001b[38;5;241m=\u001b[39m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdelimiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1374\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconverters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconverters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskiplines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskiprows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musecols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musecols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1375\u001b[0m \u001b[43m            \u001b[49m\u001b[43munpack\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munpack\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mndmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mndmin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1376\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_rows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_rows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquotechar\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arr\n",
      "File \u001b[1;32mc:\\Users\\matti\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\lib\\npyio.py:992\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(fname, delimiter, comment, quote, imaginary_unit, usecols, skiplines, max_rows, converters, ndmin, unpack, dtype, encoding)\u001b[0m\n\u001b[0;32m    990\u001b[0m     fname \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mfspath(fname)\n\u001b[0;32m    991\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fname, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m--> 992\u001b[0m     fh \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_datasource\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    993\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m encoding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    994\u001b[0m         encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(fh, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatin1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\matti\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\lib\\_datasource.py:193\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(path, mode, destpath, encoding, newline)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;124;03mOpen `path` with `mode` and return the file object.\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    189\u001b[0m \n\u001b[0;32m    190\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    192\u001b[0m ds \u001b[38;5;241m=\u001b[39m DataSource(destpath)\n\u001b[1;32m--> 193\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnewline\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\matti\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\lib\\_datasource.py:533\u001b[0m, in \u001b[0;36mDataSource.open\u001b[1;34m(self, path, mode, encoding, newline)\u001b[0m\n\u001b[0;32m    530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _file_openers[ext](found, mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[0;32m    531\u001b[0m                               encoding\u001b[38;5;241m=\u001b[39mencoding, newline\u001b[38;5;241m=\u001b[39mnewline)\n\u001b[0;32m    532\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 533\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: ((sp27:1.250340508,sp28:1.250340508)nd15:7.74146153,((((sp30:0.6971156603,(sp44:0.05448322787,sp45:0.05448322787)nd43:0.6426324324)nd39:0.6993506711,sp26:1.396466331)nd17:4.57236873,(((((sp39:0.2613079878,sp40:0.2613079878)nd40:0.7983119657,(sp31:0.6836493874,sp32:0.6836493874)nd41:0.3759705661)nd36:2.581591996,(sp21:2.047336495,(sp33:0.5882194838,sp34:0.5882194838)nd38:1.459117012)nd30:1.593875454)nd23:0.2542306771,((sp41:0.2506839765,(sp42:0.2329470068,sp43:0.2329470068)nd46:0.01773696969)nd44:0.2999376771,sp35:0.5506216536)nd24:3.344820973)nd22:0.4035940053,((sp16:2.622670915,(sp24:1.438818684,sp25:1.438818684)nd37:1.183852231)nd25:1.205457832,(((sp46:0.01563795384,sp47:0.01563795384)nd45:0.458918388,sp38:0.4745563419)nd42:0.3877672302,sp29:0.862323572)nd32:2.965805175)nd21:0.470907884)nd13:1.66979843)nd10:1.06711515,((((sp22:1.711465293,sp23:1.711465293)nd34:1.935683526,(sp36:0.5114053313,sp37:0.5114053313)nd33:3.135743488)nd19:0.7528428002,sp10:4.39999162)nd14:1.449883434,sp3:5.849875054)nd11:1.186075157)nd6:1.955851826).csv not found."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ete3 import Tree\n",
    "\n",
    "class TreeEncoder:\n",
    "    \n",
    "    def __init__(self, max_length=512):\n",
    "        self.max_length = max_length\n",
    "        self.node_feature_size = 32  # Number of features to encode for each node in the tree\n",
    "\n",
    "    def parse_newick(self, newick_str):\n",
    "        \"\"\"\n",
    "        Parse Newick tree string using ete3, handling node labels.\n",
    "        \"\"\"\n",
    "        return Tree(newick_str, format=1)\n",
    "\n",
    "    def traverse_tree(self, tree, node=None):\n",
    "        \"\"\"\n",
    "        Recursively traverse the tree to gather information about nodes and edges.\n",
    "        \"\"\"\n",
    "        if node is None:\n",
    "            node = tree.get_tree_root()\n",
    "\n",
    "        if node.is_leaf():\n",
    "            return [node.name]\n",
    "        else:\n",
    "            children = [self.traverse_tree(tree, child) for child in node.children]\n",
    "            return children\n",
    "\n",
    "    def flatten_tree_info(self, tree_info):\n",
    "        \"\"\"\n",
    "        Flatten the nested tree structure info into a 1D array.\n",
    "        \"\"\"\n",
    "        flattened = []\n",
    "        for node_info in tree_info:\n",
    "            if isinstance(node_info, list):\n",
    "                flattened.extend(self.flatten_tree_info(node_info))\n",
    "            else:\n",
    "                flattened.append(node_info)\n",
    "        return flattened\n",
    "\n",
    "    def encode_tree(self, tree_str):\n",
    "        \"\"\"\n",
    "        Encode the tree structure into a format suitable for input into the neural network.\n",
    "        \"\"\"\n",
    "        # Call the external script to get CDV encoding\n",
    "        cmd = f\"CDV_full_tree.py -t {tree_str}\"\n",
    "        os.system(cmd)\n",
    "        \n",
    "        # Now read the generated CSV file\n",
    "        csv_file = f\"{tree_str[:-4]}.csv\"\n",
    "        encoded_tree = np.loadtxt(csv_file, delimiter=\",\")\n",
    "        \n",
    "        # Pad or truncate to match max_length\n",
    "        if len(encoded_tree) < self.max_length:\n",
    "            padding = np.zeros((self.max_length - len(encoded_tree), encoded_tree.shape[1]))\n",
    "            encoded_tree = np.concatenate((encoded_tree, padding), axis=0)\n",
    "        elif len(encoded_tree) > self.max_length:\n",
    "            encoded_tree = encoded_tree[:self.max_length, :]\n",
    "        \n",
    "        return encoded_tree\n",
    "\n",
    "\n",
    "def load_data(trees_directory):\n",
    "    \"\"\"\n",
    "    Load tree and parameter files from the specified directory.\n",
    "    \"\"\"\n",
    "    tree_files = [os.path.join(trees_directory, file) for file in os.listdir(trees_directory) if file.endswith('.nwk')]\n",
    "    parameter_files = [os.path.join(trees_directory, file[:-4] + '.params') for file in os.listdir(trees_directory) if file.endswith('.nwk')]\n",
    "    return tree_files, parameter_files\n",
    "\n",
    "def process_data(tree_files, parameter_files):\n",
    "    \"\"\"\n",
    "    Process tree and parameter data.\n",
    "    \"\"\"\n",
    "    encoder = TreeEncoder()\n",
    "    X, y = [], []\n",
    "    for tree_file, param_file in zip(tree_files, parameter_files):\n",
    "        with open(tree_file, 'r') as f:\n",
    "            tree_str = f.read().strip()\n",
    "            x = encoder.encode_tree(tree_str)\n",
    "            print(\"Encoded tree shape:\", x.shape)  # Print the shape of the encoded tree for debugging\n",
    "            X.append(x)\n",
    "        \n",
    "        with open(param_file, 'r') as f:\n",
    "            params = f.readlines()\n",
    "            num_states = int(params[0].split()[1])\n",
    "            lambdas = [float(row.split()[1]) for row in params[1:1 + num_states]]\n",
    "            mus = [float(row.split()[1]) for row in params[1 + num_states:1 + 2 * num_states]]\n",
    "            qs = [float(row.split()[1]) for row in params[1 + 2 * num_states:]]\n",
    "\n",
    "            y.append(np.concatenate([lambdas, mus, qs]))\n",
    "\n",
    "    X = torch.tensor(X, dtype=torch.float32)\n",
    "    y = torch.tensor(y, dtype=torch.float32)\n",
    "    y = y / y.sum(dim=-1, keepdim=True)  # Normalize y values\n",
    "\n",
    "    return X, y\n",
    "\n",
    "# Load data\n",
    "trees_directory = \"trees/musse\"\n",
    "tree_files, parameter_files = load_data(trees_directory)\n",
    "\n",
    "# Process data\n",
    "X, y = process_data(tree_files, parameter_files)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BiSSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"list\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 74\u001b[0m\n\u001b[0;32m     71\u001b[0m trees, parameters \u001b[38;5;241m=\u001b[39m load_data_from_directory(trees_directory)\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# Preprocess data\u001b[39;00m\n\u001b[1;32m---> 74\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# Split data into training and testing sets\u001b[39;00m\n\u001b[0;32m     77\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m split_data(X, y)\n",
      "Cell \u001b[1;32mIn[28], line 41\u001b[0m, in \u001b[0;36mpreprocess_data\u001b[1;34m(tree_files, parameter_files)\u001b[0m\n\u001b[0;32m     38\u001b[0m max_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mlen\u001b[39m(tree) \u001b[38;5;28;01mfor\u001b[39;00m tree \u001b[38;5;129;01min\u001b[39;00m encoded_trees)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Pad or truncate the encoded trees to the maximum length\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m X \u001b[38;5;241m=\u001b[39m [[node \u001b[38;5;241m+\u001b[39m [\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m (max_length \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(node)) \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m tree] \u001b[38;5;28;01mfor\u001b[39;00m tree \u001b[38;5;129;01min\u001b[39;00m encoded_trees]\n\u001b[0;32m     42\u001b[0m X \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(X, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Extract BiSSE parameters\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[28], line 41\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     38\u001b[0m max_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mlen\u001b[39m(tree) \u001b[38;5;28;01mfor\u001b[39;00m tree \u001b[38;5;129;01min\u001b[39;00m encoded_trees)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Pad or truncate the encoded trees to the maximum length\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m X \u001b[38;5;241m=\u001b[39m [[node \u001b[38;5;241m+\u001b[39m [\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m (max_length \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(node)) \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m tree] \u001b[38;5;28;01mfor\u001b[39;00m tree \u001b[38;5;129;01min\u001b[39;00m encoded_trees]\n\u001b[0;32m     42\u001b[0m X \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(X, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Extract BiSSE parameters\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[28], line 41\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     38\u001b[0m max_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mlen\u001b[39m(tree) \u001b[38;5;28;01mfor\u001b[39;00m tree \u001b[38;5;129;01min\u001b[39;00m encoded_trees)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Pad or truncate the encoded trees to the maximum length\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m X \u001b[38;5;241m=\u001b[39m [[\u001b[43mnode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m tree] \u001b[38;5;28;01mfor\u001b[39;00m tree \u001b[38;5;129;01min\u001b[39;00m encoded_trees]\n\u001b[0;32m     42\u001b[0m X \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(X, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Extract BiSSE parameters\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: can only concatenate str (not \"list\") to str"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ete3 import Tree\n",
    "\n",
    "# Load trees and corresponding parameters from the specified directory\n",
    "def load_data_from_directory(directory):\n",
    "    \"\"\"\n",
    "    Load trees and corresponding parameters from the specified directory.\n",
    "    \"\"\"\n",
    "    tree_files = []\n",
    "    parameter_files = []\n",
    "\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(\".nwk\"):\n",
    "                # Load tree file\n",
    "                file_path = os.path.join(root, file)\n",
    "                with open(file_path, \"r\") as f:\n",
    "                    tree_str = f.read().strip()\n",
    "                    tree_files.append(tree_str)\n",
    "\n",
    "                # Load corresponding parameter file\n",
    "                param_file = os.path.splitext(file)[0] + \".params\"\n",
    "                param_path = os.path.join(root, param_file)\n",
    "                parameter_files.append(param_path)\n",
    "\n",
    "    return tree_files, parameter_files\n",
    "\n",
    "# Preprocess data: encode tree structure and labels into numerical format for training\n",
    "def preprocess_data(tree_files, parameter_files):\n",
    "    \"\"\"\n",
    "    Preprocess data: encode tree structure and labels into numerical format for training.\n",
    "    \"\"\"\n",
    "    # Encode trees and determine the maximum length\n",
    "    encoded_trees = [encode_tree(tree) for tree in tree_files]\n",
    "    max_length = max(len(tree) for tree in encoded_trees)\n",
    "    \n",
    "    # Pad or truncate the encoded trees to the maximum length\n",
    "    X = [[node + [0] * (max_length - len(node)) for node in tree] for tree in encoded_trees]\n",
    "    X = torch.tensor(X, dtype=torch.float32)\n",
    "    \n",
    "    # Extract BiSSE parameters\n",
    "    y = []\n",
    "    for param_file in parameter_files:\n",
    "        with open(param_file, \"r\") as f:\n",
    "            param_lines = f.readlines()\n",
    "            param_dict = {}\n",
    "            for line in param_lines:\n",
    "                key, value = line.strip().split(\"=\")\n",
    "                param_dict[key.strip()] = float(value.strip())\n",
    "            y.append([param_dict[\"lambda1\"], param_dict[\"lambda2\"], param_dict[\"mu1\"], param_dict[\"mu2\"], param_dict[\"q12\"], param_dict[\"q21\"]])\n",
    "\n",
    "    y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "# Split data into training and testing sets\n",
    "def split_data(X, y, test_size=0.2):\n",
    "    \"\"\"\n",
    "    Split data into training and testing sets.\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# Load data from the specified directory\n",
    "trees_directory = \"trees/bisse\"\n",
    "\n",
    "# Load trees and parameters\n",
    "trees, parameters = load_data_from_directory(trees_directory)\n",
    "\n",
    "# Preprocess data\n",
    "X, y = preprocess_data(trees, parameters)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = split_data(X, y)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
